{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Parametrizations Tutorial\n",
        "**Author**: [Mario Lezcano](https://github.com/lezcano)\n",
        "\n",
        "Regularizing deep-learning models is a surprisingly challenging task.\n",
        "Classical techniques such as penalty methods often fall short when applied\n",
        "on deep models due to the complexity of the function being optimized.\n",
        "This is particularly problematic when working with ill-conditioned models.\n",
        "Examples of these are RNNs trained on long sequences and GANs. A number\n",
        "of techniques have been proposed in recent years to regularize these\n",
        "models and improve their convergence. On recurrent models, it has been\n",
        "proposed to control the singular values of the recurrent kernel for the\n",
        "RNN to be well-conditioned. This can be achieved, for example, by making\n",
        "the recurrent kernel [orthogonal](https://en.wikipedia.org/wiki/Orthogonal_matrix).\n",
        "Another way to regularize recurrent models is via\n",
        "\"[weight normalization](https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html)\".\n",
        "This approach proposes to decouple the learning of the parameters from the\n",
        "learning of their norms.  To do so, the parameter is divided by its\n",
        "[Frobenius norm](https://en.wikipedia.org/wiki/Matrix_norm#Frobenius_norm)\n",
        "and a separate parameter encoding its norm is learnt.\n",
        "A similar regularization was proposed for GANs under the name of\n",
        "\"[spectral normalization](https://pytorch.org/docs/stable/generated/torch.nn.utils.spectral_norm.html)\". This method\n",
        "controls the Lipschitz constant of the network by dividing its parameters by\n",
        "their [spectral norm](https://en.wikipedia.org/wiki/Matrix_norm#Special_cases),\n",
        "rather than their Frobenius norm.\n",
        "\n",
        "All these methods have a common pattern: they all transform a parameter\n",
        "in an appropriate way before using it. In the first case, they make it orthogonal by\n",
        "using a function that maps matrices to orthogonal matrices. In the case of weight\n",
        "and spectral normalization, they divide the original parameter by its norm.\n",
        "\n",
        "More generally, all these examples use a function to put extra structure on the parameters.\n",
        "In other words, they use a function to constrain the parameters.\n",
        "\n",
        "In this tutorial, you will learn how to implement and use this pattern to put\n",
        "constraints on your model. Doing so is as easy as writing your own ``nn.Module``.\n",
        "\n",
        "Requirements: ``torch>=1.9.0``\n",
        "\n",
        "## Implementing parametrizations by hand\n",
        "\n",
        "Assume that we want to have a square linear layer with symmetric weights, that is,\n",
        "with weights ``X`` such that ``X = Xáµ€``. One way to do so is\n",
        "to copy the upper-triangular part of the matrix into its lower-triangular part\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2371, 0.1048, 0.5547],\n",
            "        [0.1048, 0.2058, 0.5875],\n",
            "        [0.5547, 0.5875, 0.1975]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.parametrize as parametrize\n",
        "\n",
        "def symmetric(X):\n",
        "    return X.triu() + X.triu(1).transpose(-1, -2)\n",
        "\n",
        "X = torch.rand(3, 3)\n",
        "A = symmetric(X)\n",
        "assert torch.allclose(A, A.T)  # A is symmetric\n",
        "print(A)                       # Quick visual check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then use this idea to implement a linear layer with symmetric weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class LinearSymmetric(nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.rand(n_features, n_features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        A = symmetric(self.weight)\n",
        "        return x @ A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The layer can be then used as a regular linear layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "layer = LinearSymmetric(3)\n",
        "out = layer(torch.rand(8, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This implementation, although correct and self-contained, presents a number of problems:\n",
        "\n",
        "1) It reimplements the layer. We had to implement the linear layer as ``x @ A``. This is\n",
        "   not very problematic for a linear layer, but imagine having to reimplement a CNN or a\n",
        "   Transformer...\n",
        "2) It does not separate the layer and the parametrization.  If the parametrization were\n",
        "   more difficult, we would have to rewrite its code for each layer that we want to use it\n",
        "   in.\n",
        "3) It recomputes the parametrization everytime we use the layer. If we use the layer\n",
        "   several times during the forward pass, (imagine the recurrent kernel of an RNN), it\n",
        "   would compute the same ``A`` every time that the layer is called.\n",
        "\n",
        "## Introduction to parametrizations\n",
        "\n",
        "Parametrizations can solve all these problems as well as others.\n",
        "\n",
        "Let's start by reimplementing the code above using ``torch.nn.utils.parametrize``.\n",
        "The only thing that we have to do is to write the parametrization as a regular ``nn.Module``\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Symmetric(nn.Module):\n",
        "    def forward(self, X):\n",
        "        return X.triu() + X.triu(1).transpose(-1, -2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is all we need to do. Once we have this, we can transform any regular layer into a\n",
        "symmetric layer by doing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ParametrizedLinear(\n",
              "  in_features=3, out_features=3, bias=True\n",
              "  (parametrizations): ModuleDict(\n",
              "    (weight): ParametrizationList(\n",
              "      (0): Symmetric()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "layer = nn.Linear(3, 3)\n",
        "parametrize.register_parametrization(layer, \"weight\", Symmetric())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, the matrix of the linear layer is symmetric\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.3753, -0.5307, -0.3252],\n",
            "        [-0.5307, -0.3520, -0.4990],\n",
            "        [-0.3252, -0.4990,  0.4180]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "A = layer.weight\n",
        "assert torch.allclose(A, A.T)  # A is symmetric\n",
        "print(A)                       # Quick visual check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can do the same thing with any other layer. For example, we can create a CNN with\n",
        "[skew-symmetric](https://en.wikipedia.org/wiki/Skew-symmetric_matrix) kernels.\n",
        "We use a similar parametrization, copying the upper-triangular part with signs\n",
        "reversed into the lower-triangular part\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000, -0.1121, -0.0147],\n",
            "        [ 0.1121,  0.0000,  0.0085],\n",
            "        [ 0.0147, -0.0085,  0.0000]], grad_fn=<SelectBackward0>)\n",
            "tensor([[ 0.0000,  0.0334, -0.1376],\n",
            "        [-0.0334,  0.0000, -0.0463],\n",
            "        [ 0.1376,  0.0463,  0.0000]], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "class Skew(nn.Module):\n",
        "    def forward(self, X):\n",
        "        A = X.triu(1)\n",
        "        return A - A.transpose(-1, -2)\n",
        "\n",
        "\n",
        "cnn = nn.Conv2d(in_channels=5, out_channels=8, kernel_size=3)\n",
        "parametrize.register_parametrization(cnn, \"weight\", Skew())\n",
        "# Print a few kernels\n",
        "print(cnn.weight[0, 1])\n",
        "print(cnn.weight[2, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspecting a parametrized module\n",
        "\n",
        "When a module is parametrized, we find that the module has changed in three ways:\n",
        "\n",
        "1) ``model.weight`` is now a property\n",
        "\n",
        "2) It has a new ``module.parametrizations`` attribute\n",
        "\n",
        "3) The unparametrized weight has been moved to ``module.parametrizations.weight.original``\n",
        "\n",
        "|\n",
        "After parametrizing ``weight``, ``layer.weight`` is turned into a\n",
        "[Python property](https://docs.python.org/3/library/functions.html#property).\n",
        "This property computes ``parametrization(weight)`` every time we request ``layer.weight``\n",
        "just as we did in our implementation of ``LinearSymmetric`` above.\n",
        "\n",
        "Registered parametrizations are stored under a ``parametrizations`` attribute within the module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unparametrized:\n",
            "Linear(in_features=3, out_features=3, bias=True)\n",
            "\n",
            "Parametrized:\n",
            "ParametrizedLinear(\n",
            "  in_features=3, out_features=3, bias=True\n",
            "  (parametrizations): ModuleDict(\n",
            "    (weight): ParametrizationList(\n",
            "      (0): Symmetric()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "layer = nn.Linear(3, 3)\n",
        "print(f\"Unparametrized:\\n{layer}\")\n",
        "parametrize.register_parametrization(layer, \"weight\", Symmetric())\n",
        "print(f\"\\nParametrized:\\n{layer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This ``parametrizations`` attribute is an ``nn.ModuleDict``, and it can be accessed as such\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ModuleDict(\n",
            "  (weight): ParametrizationList(\n",
            "    (0): Symmetric()\n",
            "  )\n",
            ")\n",
            "ParametrizationList(\n",
            "  (0): Symmetric()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(layer.parametrizations)\n",
        "print(layer.parametrizations.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each element of this ``nn.ModuleDict`` is a ``ParametrizationList``, which behaves like an\n",
        "``nn.Sequential``. This list will allow us to concatenate parametrizations on one weight.\n",
        "Since this is a list, we can access the parametrizations indexing it. Here's\n",
        "where our ``Symmetric`` parametrization sits\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Symmetric()\n"
          ]
        }
      ],
      "source": [
        "print(layer.parametrizations.weight[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The other thing that we notice is that, if we print the parameters, we see that the\n",
        "parameter ``weight`` has been moved\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'bias': Parameter containing:\n",
            "tensor([-0.5322,  0.3347,  0.0254], requires_grad=True), 'parametrizations.weight.original': Parameter containing:\n",
            "tensor([[-0.3164,  0.3332,  0.0700],\n",
            "        [ 0.2474, -0.5238, -0.5490],\n",
            "        [-0.2337,  0.4060,  0.1218]], requires_grad=True)}\n"
          ]
        }
      ],
      "source": [
        "print(dict(layer.named_parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It now sits under ``layer.parametrizations.weight.original``\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.3164,  0.3332,  0.0700],\n",
            "        [ 0.2474, -0.5238, -0.5490],\n",
            "        [-0.2337,  0.4060,  0.1218]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "print(layer.parametrizations.weight.original)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Besides these three small differences, the parametrization is doing exactly the same\n",
        "as our manual implementation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0., grad_fn=<DistBackward0>)\n"
          ]
        }
      ],
      "source": [
        "symmetric = Symmetric()\n",
        "weight_orig = layer.parametrizations.weight.original\n",
        "print(torch.dist(layer.weight, symmetric(weight_orig)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parametrizations are first-class citizens\n",
        "\n",
        "Since ``layer.parametrizations`` is an ``nn.ModuleList``, it means that the parametrizations\n",
        "are properly registered as submodules of the original module. As such, the same rules\n",
        "for registering parameters in a module apply to register a parametrization.\n",
        "For example, if a parametrization has parameters, these will be moved from CPU\n",
        "to CUDA when calling ``model = model.cuda()``.\n",
        "\n",
        "## Caching the value of a parametrization\n",
        "\n",
        "Parametrizations come with an inbuilt caching system via the context manager\n",
        "``parametrize.cached()``\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing the Parametrization\n",
            "Here, layer.weight is recomputed every time we call it\n",
            "Computing the Parametrization\n",
            "Computing the Parametrization\n",
            "Computing the Parametrization\n",
            "Here, it is computed just the first time layer.weight is called\n",
            "Computing the Parametrization\n"
          ]
        }
      ],
      "source": [
        "class NoisyParametrization(nn.Module):\n",
        "    def forward(self, X):\n",
        "        print(\"Computing the Parametrization\")\n",
        "        return X\n",
        "\n",
        "layer = nn.Linear(4, 4)\n",
        "parametrize.register_parametrization(layer, \"weight\", NoisyParametrization())\n",
        "print(\"Here, layer.weight is recomputed every time we call it\")\n",
        "foo = layer.weight + layer.weight.T\n",
        "bar = layer.weight.sum()\n",
        "with parametrize.cached():\n",
        "    print(\"Here, it is computed just the first time layer.weight is called\")\n",
        "    foo = layer.weight + layer.weight.T\n",
        "    bar = layer.weight.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Concatenating parametrizations\n",
        "\n",
        "Concatenating two parametrizations is as easy as registering them on the same tensor.\n",
        "We may use this to create more complex parametrizations from simpler ones. For example, the\n",
        "[Cayley map](https://en.wikipedia.org/wiki/Cayley_transform#Matrix_map)\n",
        "maps the skew-symmetric matrices to the orthogonal matrices of positive determinant. We can\n",
        "concatenate ``Skew`` and a parametrization that implements the Cayley map to get a layer with\n",
        "orthogonal weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "This function was deprecated since version 1.9 and is now removed. Please use the `torch.linalg.solve` function instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [16], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m layer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m     11\u001b[0m parametrize\u001b[39m.\u001b[39mregister_parametrization(layer, \u001b[39m\"\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m\"\u001b[39m, Skew())\n\u001b[1;32m---> 12\u001b[0m parametrize\u001b[39m.\u001b[39;49mregister_parametrization(layer, \u001b[39m\"\u001b[39;49m\u001b[39mweight\u001b[39;49m\u001b[39m\"\u001b[39;49m, CayleyMap(\u001b[39m3\u001b[39;49m))\n\u001b[0;32m     13\u001b[0m X \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mweight\n\u001b[0;32m     14\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39mdist(X\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m X, torch\u001b[39m.\u001b[39meye(\u001b[39m3\u001b[39m)))\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\utils\\parametrize.py:479\u001b[0m, in \u001b[0;36mregister_parametrization\u001b[1;34m(module, tensor_name, parametrization, unsafe)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m unsafe:\n\u001b[0;32m    478\u001b[0m     Y \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(module, tensor_name)\n\u001b[1;32m--> 479\u001b[0m     X \u001b[39m=\u001b[39m parametrization(Y)\n\u001b[0;32m    480\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(X, Tensor):\n\u001b[0;32m    481\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    482\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mA parametrization must return a tensor. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(X)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m         )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[1;32mIn [16], line 8\u001b[0m, in \u001b[0;36mCayleyMap.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m      7\u001b[0m     \u001b[39m# (I + X)(I - X)^{-1}\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49msolve(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mId \u001b[39m+\u001b[39;49m X, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mId \u001b[39m-\u001b[39;49m X)\u001b[39m.\u001b[39msolution\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\_linalg_utils.py:106\u001b[0m, in \u001b[0;36msolve\u001b[1;34m(input, A, out)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msolve\u001b[39m(\u001b[39minput\u001b[39m: Tensor, A: Tensor, \u001b[39m*\u001b[39m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[1;32m--> 106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    107\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis function was deprecated since version 1.9 and is now removed. Please use the `torch.linalg.solve` function instead.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    108\u001b[0m     )\n",
            "\u001b[1;31mRuntimeError\u001b[0m: This function was deprecated since version 1.9 and is now removed. Please use the `torch.linalg.solve` function instead."
          ]
        }
      ],
      "source": [
        "class CayleyMap(nn.Module):\n",
        "    def __init__(self, n):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"Id\", torch.eye(n))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # (I + X)(I - X)^{-1}\n",
        "        return torch.solve(self.Id + X, self.Id - X).solution\n",
        "\n",
        "layer = nn.Linear(3, 3)\n",
        "parametrize.register_parametrization(layer, \"weight\", Skew())\n",
        "parametrize.register_parametrization(layer, \"weight\", CayleyMap(3))\n",
        "X = layer.weight\n",
        "print(torch.dist(X.T @ X, torch.eye(3)))  # X is orthogonal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This may also be used to prune a parametrized module, or to reuse parametrizations. For example,\n",
        "the matrix exponential maps the symmetric matrices to the Symmetric Positive Definite (SPD) matrices\n",
        "But the matrix exponential also maps the skew-symmetric matrices to the orthogonal matrices.\n",
        "Using these two facts, we may reuse the parametrizations before to our advantage\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MatrixExponential(nn.Module):\n",
        "    def forward(self, X):\n",
        "        return torch.matrix_exp(X)\n",
        "\n",
        "layer_orthogonal = nn.Linear(3, 3)\n",
        "parametrize.register_parametrization(layer_orthogonal, \"weight\", Skew())\n",
        "parametrize.register_parametrization(layer_orthogonal, \"weight\", MatrixExponential())\n",
        "X = layer_orthogonal.weight\n",
        "print(torch.dist(X.T @ X, torch.eye(3)))         # X is orthogonal\n",
        "\n",
        "layer_spd = nn.Linear(3, 3)\n",
        "parametrize.register_parametrization(layer_spd, \"weight\", Symmetric())\n",
        "parametrize.register_parametrization(layer_spd, \"weight\", MatrixExponential())\n",
        "X = layer_spd.weight\n",
        "print(torch.dist(X, X.T))                        # X is symmetric\n",
        "print((torch.symeig(X).eigenvalues > 0.).all())  # X is positive definite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Intializing parametrizations\n",
        "\n",
        "Parametrizations come with a mechanism to initialize them. If we implement a method\n",
        "``right_inverse`` with signature\n",
        "\n",
        "```python\n",
        "def right_inverse(self, X: Tensor) -> Tensor\n",
        "```\n",
        "it will be used when assigning to the parametrized tensor.\n",
        "\n",
        "Let's upgrade our implementation of the ``Skew`` class to support this\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Skew(nn.Module):\n",
        "    def forward(self, X):\n",
        "        A = X.triu(1)\n",
        "        return A - A.transpose(-1, -2)\n",
        "\n",
        "    def right_inverse(self, A):\n",
        "        # We assume that A is skew-symmetric\n",
        "        # We take the upper-triangular elements, as these are those used in the forward\n",
        "        return A.triu(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We may now initialize a layer that is parametrized with ``Skew``\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "layer = nn.Linear(3, 3)\n",
        "parametrize.register_parametrization(layer, \"weight\", Skew())\n",
        "X = torch.rand(3, 3)\n",
        "X = X - X.T                             # X is now skew-symmetric\n",
        "layer.weight = X                        # Initialize layer.weight to be X\n",
        "print(torch.dist(layer.weight, X))      # layer.weight == X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This ``right_inverse`` works as expected when we concatenate parametrizations.\n",
        "To see this, let's upgrade the Cayley parametrization to also support being initialized\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class CayleyMap(nn.Module):\n",
        "    def __init__(self, n):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"Id\", torch.eye(n))\n",
        "\n",
        "    def forward(self, X):\n",
        "        # Assume X skew-symmetric\n",
        "        # (I + X)(I - X)^{-1}\n",
        "        return torch.solve(self.Id + X, self.Id - X).solution\n",
        "\n",
        "    def right_inverse(self, A):\n",
        "        # Assume A orthogonal\n",
        "        # See https://en.wikipedia.org/wiki/Cayley_transform#Matrix_map\n",
        "        # (X - I)(X + I)^{-1}\n",
        "        return torch.solve(X - self.Id, self.Id + X).solution\n",
        "\n",
        "layer_orthogonal = nn.Linear(3, 3)\n",
        "parametrize.register_parametrization(layer_orthogonal, \"weight\", Skew())\n",
        "parametrize.register_parametrization(layer_orthogonal, \"weight\", CayleyMap(3))\n",
        "# Sample an orthogonal matrix with positive determinant\n",
        "X = torch.empty(3, 3)\n",
        "nn.init.orthogonal_(X)\n",
        "if X.det() < 0.:\n",
        "    X[0].neg_()\n",
        "layer_orthogonal.weight = X\n",
        "print(torch.dist(layer_orthogonal.weight, X))  # layer_orthogonal.weight == X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This initialization step can be written more succinctly as\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "layer_orthogonal.weight = nn.init.orthogonal_(layer_orthogonal.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The name of this method comes from the fact that we would often expect\n",
        "that ``forward(right_inverse(X)) == X``. This is a direct way of rewriting that\n",
        "the forward afer the initalization with value ``X`` should return the value ``X``.\n",
        "This constraint is not strongly enforced in practice. In fact, at times, it might be of\n",
        "interest to relax this relation. For example, consider the following implementation\n",
        "of a randomized pruning method:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class PruningParametrization(nn.Module):\n",
        "    def __init__(self, X, p_drop=0.2):\n",
        "        super().__init__()\n",
        "        # sample zeros with probability p_drop\n",
        "        mask = torch.full_like(X, 1.0 - p_drop)\n",
        "        self.mask = torch.bernoulli(mask)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return X * self.mask\n",
        "\n",
        "    def right_inverse(self, A):\n",
        "        return A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, it is not true that for every matrix A ``forward(right_inverse(A)) == A``.\n",
        "This is only true when the matrix ``A`` has zeros in the same positions as the mask.\n",
        "Even then, if we assign a tensor to a pruned parameter, it will comes as no surprise\n",
        "that tensor will be, in fact, pruned\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "layer = nn.Linear(3, 4)\n",
        "X = torch.rand_like(layer.weight)\n",
        "print(f\"Initialization matrix:\\n{X}\")\n",
        "parametrize.register_parametrization(layer, \"weight\", PruningParametrization(layer.weight))\n",
        "layer.weight = X\n",
        "print(f\"\\nInitialized weight:\\n{layer.weight}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Removing parametrizations\n",
        "\n",
        "We may remove all the parametrizations from a parameter or a buffer in a module\n",
        "by using ``parametrize.remove_parametrizations()``\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "layer = nn.Linear(3, 3)\n",
        "print(\"Before:\")\n",
        "print(layer)\n",
        "print(layer.weight)\n",
        "parametrize.register_parametrization(layer, \"weight\", Skew())\n",
        "print(\"\\nParametrized:\")\n",
        "print(layer)\n",
        "print(layer.weight)\n",
        "parametrize.remove_parametrizations(layer, \"weight\")\n",
        "print(\"\\nAfter. Weight has skew-symmetric values but it is unconstrained:\")\n",
        "print(layer)\n",
        "print(layer.weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When removing a parametrization, we may choose to leave the original parameter (i.e. that in\n",
        "``layer.parametriations.weight.original``) rather than its parametrized version by setting\n",
        "the flag ``leave_parametrized=False``\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "layer = nn.Linear(3, 3)\n",
        "print(\"Before:\")\n",
        "print(layer)\n",
        "print(layer.weight)\n",
        "parametrize.register_parametrization(layer, \"weight\", Skew())\n",
        "print(\"\\nParametrized:\")\n",
        "print(layer)\n",
        "print(layer.weight)\n",
        "parametrize.remove_parametrizations(layer, \"weight\", leave_parametrized=False)\n",
        "print(\"\\nAfter. Same as Before:\")\n",
        "print(layer)\n",
        "print(layer.weight)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "61353369d0b3b3a3144ddd818ca0f373af58956ae49fc5413563dfae05a6cee9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
